<html>
<head>
    <title>Research</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content="Luke Snow's site.">
    <!-- A decent browser will parse this fine:
         https://webmasters.stackexchange.com/questions/92744. -->
    <meta name='keywords' content='
        signal processing,
        machine learning,
        inverse reinforcement learning,
        mechanism design,
        game theory,
        deep learning,
        Langevin dynamics,
        intelligent systems
    '>
    <meta name='author' content='Luke Snow'>

    <link rel='shortcut icon' href='/assets/website/website.png' />
    <link href='/css/blog.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['[[', ']]']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/about'>About</a></li>
        <li><a href='/research'>Research</a></li>
        <li><a href='/publications'>Publications</a></li>
        <li><a href='/personal'>Personal</a></li>
    </ul>
</div>

    <div id='blog' class='wrap'>

    <h2>Research</h2>
        
         I enjoy exploring sophisticated mathematical tools in stochastic analysis, in e.g., diffusion-operator theory, stochastic calculus and rough path theory, 
        in order to enhance the analysis and development of modern statistical machine learning and signal processing algorithms. My work finds application in:
          <ul>
            <li>generative modeling</li>
            <li>inverse reinforcement learning</li>
            <li>algorithmic game theory</li>
          </ul>
        
        <p>
            Below are a few examples, see my <a href='/publications'>publications</a> for a full list.

        </p> 
        
    <br> 
    <hr>    
        <section class="research-highlight" aria-labelledby="nsde-cubature-title">
      <h3 id="nsde-cubature-title">Efficient Neural SDE Training via Wiener–Space Cubature</h3>

            <img src="/assets/nsde_train.png" 
                style="width: 100%"/> 
            
      <p>
        Neural stochastic differential equations (SDEs) parametrize SDE drift and diffusion vector fields as neural networks, 
        providing an expressive model for time-series generative modeling.
        They are trained by minimizing an expected loss over path space, capturing the discrepancy between induced sample paths and time-series data.
        We introduce a cubature-based training method that replaces Monte Carlo neural SDE gradient evaluations with
        weighted evaluations of <i>deterministic ODE solutions</i>. 
      </p>
      <p>    
        This is a fundamental paradigm shift, enabling
        ultra-efficient neural SDE training by bypassing Brownian path computations, exploiting memory-efficient parallel ODE solvers,
        and reducing the number of paths needed for a given accuracy. Specifically, we show that this method achieves  <em>O</em>(n<sup>−1</sup>) convergence in path evaluations,
            outperforming naïve Monte Carlo (<em>O</em>(n<sup>−1/2</sup>)) and quasi–Monte Carlo (<em>O</em>((log n)/n)).
      </p>
   <center><img src="/assets/convg_time5.png" 
                style="width: 50%"/> </center> 
      <p>
        Mathematically, these guarantees are enabled by novel extensions of Wiener–space cubature guarantees to
        <i>Lipschitz–nonlinear path functionals</i> necessary for diffusion training, yielding rigorous error bounds
        for approximating expected objectives by deterministic ODE evaluations. 
      </p>
            


        <span class="authors">Luke Snow, Vikram Krishnamurthy  </span><br>
        <span class="title">Efficient Neural SDE Training using Wiener-Space Cubature </span><br>
        <span class="venue">arXiv:2502.12395</span>, 2025
        <a class='label pdf-label' href='https://arxiv.org/abs/2502.12395'>[pdf]</a>
    </section>
        
    <br> 
    <hr>
        
         <section class="research-highlight" aria-labelledby="passive-langevin-dynamics">
      <h3 id="nsde-cubature-title">Passive Langevin Dynamics for Adaptive Inverse Reinforcement Learning</h3>
        <p> 
            <img src="/assets/PSGLD.png" 
                style="width: 80%"/> 
        </p>
        <p>
        Suppose an agent enacts a stochastic optimization algorithm to minimize a cost function. By observing sequential noisy gradients from this process, how can 
        we (an external observer) reconstruct this cost function in its entirety? This is a ganeralized transient-learning analog of inverse reinforcement learning (IRL), in which the reward function 
        of a reinforcement learning agent is recovered by observing optimal policy evaluations.</p>
        <p>

        We develop a <i>passive</i> stochastic gradient Langevin dynamics (PSGLD) algorithm which accomplishes this adaptive IRL objective, and provide sample complexity bounds for the cost function reconstruction accuracy,  
        exploiting tools in the theory of Markov diffusion operators. 
        </p>
        <p> 
            <img src="/assets/CMDP_plot_a.png" 
                style="width: 48%"/>  <img src="/assets/CMDP_plot_b.png" 
                style="width: 48%"/> <br>
           <center><img src="/assets/CMDP_plot_c.png" 
                style="width: 100%"/> </center> 
        </p> 
             

            <span class="authors">Luke Snow, Vikram Krishnamurthy  </span><br>
            <span class="title">Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics </span><br>
            <span class="venue">IEEE Transactions on Information Theory</span>, 2025
            <a class='label pdf-label' href='https://ieeexplore.ieee.org/abstract/document/10943244'>[pdf]</a>
    </section>
        
    <br> 

    <hr>
        <section class="research-highlight" aria-labelledby="mechanism-design">
      <h3 id="nsde-cubature-title">Data-Driven Mechanism Design using Multi-Agent Revealed Preferences</h3>

        <p> 
           <center><img src="/assets/MD.png" 
                style="width: 45%"/> </center> 
        </p>
        <p>
                The stable behavior of strategically interacting multi-agent systems is captured by equilibria concepts such as Nash equilibria. However, non-cooperative strategic equilibria often degrade
            the performance (utility attained) of each agent in the system. A goal of mechanism design is to fashion the game structure (mapping from agent actions to outcomes) such that 
            <em> non-cooperative interactions lead to outcomes which maximize the performance (utility) of all agents</em>. 
        </p>
        
        <p>
            We provide a novel algorithmic approach to accomplishing mechanism design adaptively, by iteratively interacting with the system and observing Nash equilibria. Our framework allows for mechanism
            design to be achieved <em> even when the designer has no observation of the agent utilities</em>. We exploit tools from microeconomic revealed preference theory.

       </p>
              <center><img src="/assets/AMD.png" 
                style="width: 40%"/> </center> 
        <p> 

            <span class="authors">Luke Snow, Vikram Krishnamurthy </span><br>
             <span class="title">Data-Driven Mechanism Design using Multi-Agent Revealed Preferences </span><br>
             <span class="venue">arXiv:2404.15391</span>, 2024
             <a class='label pdf-label' href='https://arxiv.org/abs/2404.15391'>[pdf]</a>
         </p> 
            </section>

        <hr>
        
                
    </div>
</body>
</html>
